{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import textworld.gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_id=textworld.gym.register_game(\"/home/sirzechlucifer/tw_games/custom_game.ulx\",max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env=gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "                    ________  ________  __    __  ________\n",
      "                   |        \\|        \\|  \\  |  \\|        \\\n",
      "                    \\$$$$$$$$| $$$$$$$$| $$  | $$ \\$$$$$$$$\n",
      "                      | $$   | $$__     \\$$\\/  $$   | $$\n",
      "                      | $$   | $$  \\     >$$  $$    | $$\n",
      "                      | $$   | $$$$$    /  $$$$\\    | $$\n",
      "                      | $$   | $$_____ |  $$ \\$$\\   | $$\n",
      "                      | $$   | $$     \\| $$  | $$   | $$\n",
      "                       \\$$    \\$$$$$$$$ \\$$   \\$$    \\$$\n",
      "              __       __   ______   _______   __        _______\n",
      "             |  \\  _  |  \\ /      \\ |       \\ |  \\      |       \\\n",
      "             | $$ / \\ | $$|  $$$$$$\\| $$$$$$$\\| $$      | $$$$$$$\\\n",
      "             | $$/  $\\| $$| $$  | $$| $$__| $$| $$      | $$  | $$\n",
      "             | $$  $$$\\ $$| $$  | $$| $$    $$| $$      | $$  | $$\n",
      "             | $$ $$\\$$\\$$| $$  | $$| $$$$$$$\\| $$      | $$  | $$\n",
      "             | $$$$  \\$$$$| $$__/ $$| $$  | $$| $$_____ | $$__/ $$\n",
      "             | $$$    \\$$$ \\$$    $$| $$  | $$| $$     \\| $$    $$\n",
      "              \\$$      \\$$  \\$$$$$$  \\$$   \\$$ \\$$$$$$$$ \\$$$$$$$\n",
      "\n",
      "Welcome to another life changing game of TextWorld! Here is how to play! First\n",
      "off, if it's not too much trouble, I need you to go to the east. After that,\n",
      "make an effort to move south. Following that, retrieve the latchkey from the\n",
      "toolbox. Once you have picked up the latchkey, take a trip north. After that,\n",
      "insert the latchkey into the spherical locker. And once you've done that, you\n",
      "win!\n",
      "\n",
      "-= Scullery =-\n",
      "You find yourself in a scullery. A typical one.\n",
      "\n",
      "You scan the room for a counter, and you find a counter. The counter appears to\n",
      "be empty. It would have been so cool if there was stuff on the counter.\n",
      "\n",
      "There is a closed gateway leading south. There is an open door leading east.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs, infos = env.reset()  \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMDQN:\n",
    "    \n",
    "    def __init__(self, game, rnn_size=100, batch_size=25,seq_length=30, embed_dim=100, \n",
    "              layer_depth=3,start_epsilon=1, epsilon_end_time=1000000,memory_size=1000000, \n",
    "              checkpoint_dir=\"checkpoint\", forward_only=False):\n",
    "        self.sess=tf.Session()\n",
    "        self.rnn_size=rnn_size\n",
    "        self.batch_size=batch_size\n",
    "        self.layer_depth=layer_depth\n",
    "        self.embed_dim=embed_dim\n",
    "        self.vocab_size=100\n",
    "        self.epsilon=self.epsilon_start_time=start_epsilon\n",
    "        self.final_epsilon=0.05\n",
    "        self.observe=500\n",
    "        self.explore=500\n",
    "        self.gamma=0.99\n",
    "        self.num_action_per_step=1\n",
    "        self.memory_size=memory_size\n",
    "        self.game=game\n",
    "        self.dataset=game.name\n",
    "        self.num_action=len(self.game.actions)\n",
    "        self.num_object=len(self.game.objects)\n",
    "        self._attrs=['epsilon', 'final_epsilon', 'oberve','explore', 'gamma', 'memory_size', 'batch_size']\n",
    "        self.build_model()\n",
    "    def build_model(self):\n",
    "        self.inputs=tf.placeholder(tf.int32,[self.batch_size,self.seq_length])\n",
    "        embed=tf.get_variable(\"embed\",[self.vocab_size,self.embed_dim])\n",
    "        word_embeds=tf.nn.embedding_lookup(embed,self.inputs)\n",
    "        self.cell=tf.nn.rnn_cell(self.rnn_size)\n",
    "        self.stacked_cell=tf.nn.rnn_cell.MultiRNNCell([self.cell]*self.layer_depth)\n",
    "        outputs,_=tf.nn.rnn(self.cell,[tf.reshape(embed_t, [self.batch_size, self.embed_dim]) for embed_t in tf.split(1, self.seq_length, word_embeds)],dtype=tf.float32)\n",
    "        output_embed=tf.transpose(tf.pack(outputs),[1,0,2])\n",
    "        self.pred_reward = tf.nn.rnn_cell.linear(mean_pool, self.num_action, 0.0, scope=\"action\")\n",
    "        self.pred_object = tf.nn.rnn_cell.linear(mean_pool, self.num_object, 0.0, scope=\"object\")\n",
    "        self.true_reward = tf.placeholder(tf.float32, [self.batch_size, self.num_action])\n",
    "        self.true_object = tf.placeholder(tf.float32, [self.batch_size, self.num_object])\n",
    "        _ = tf.histogram_summary(\"mean_pool\", mean_pool)\n",
    "        _ = tf.histogram_summary(\"pred_reward\", self.pred_reward)\n",
    "        _ = tf.histogram_summary(\"true_reward\", self.true_reward)\n",
    "        _ = tf.scalar_summary(\"pred_reward_mean\", tf.reduce_mean(self.pred_reward))\n",
    "        _ = tf.scalar_summary(\"true_reward_mean\", tf.reduce_mean(self.true_reward))\n",
    "    def train(self, max_iter=1000000,\n",
    "            alpha=0.01, learning_rate=0.001,\n",
    "            start_epsilon=1.0, final_epsilon=0.05, memory_size=5000,\n",
    "            checkpoint_dir=\"checkpoint\"):\n",
    "        with self.sess:\n",
    "            self.max_iter = max_iter\n",
    "            self.alpha = alpha\n",
    "            self.learning_rate = learning_rate\n",
    "            self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "            self.step = tf.Variable(0, trainable=False)\n",
    "\n",
    "            self.loss = tf.reduce_sum(tf.square(self.true_reward - self.pred_reward))\n",
    "            self.optim = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            _ = tf.scalar_summary(\"loss\", self.loss)\n",
    "\n",
    "            self.memory = deque()\n",
    "            action = np.zeros(self.num_action)\n",
    "            action[0] = 1\n",
    "            self.initialize(log_dir=\"./logs\")\n",
    "\n",
    "            start_time = time.time()\n",
    "            start_iter = self.step.eval()\n",
    "\n",
    "            state_t, reward, is_finished = self.game.new_game()\n",
    "\n",
    "            win_count = 0\n",
    "            steps = xrange(start_iter, start_iter + self.max_iter)\n",
    "            print(\" [*] Start\")\n",
    "\n",
    "            for step in steps:\n",
    "                pred_reward, pred_object = self.sess.run([self.pred_reward, self.pred_object], feed_dict={self.inputs: [state_t]})\n",
    "                action_t = np.zeros([self.num_action])\n",
    "                object_t = np.zeros([self.num_object])\n",
    "                if random.random() <= self.epsilon or step <= self.observe:\n",
    "                    action_idx = random.randrange(0, self.num_action - 1)\n",
    "                    object_idx = random.randrange(0, self.num_action - 1)\n",
    "                else:\n",
    "                    max_reward = np.max(pred_reward[0])\n",
    "                    max_object = np.max(pred_object[0])\n",
    "                    action_idx = np.random.choice(np.where(pred_reward[0] == max_reward)[0])\n",
    "                    object_idx = np.random.choice(np.where(pred_object[0] == max_object)[0])\n",
    "                action_t[action_idx] = 1\n",
    "                object_t[object_idx] = 1\n",
    "\n",
    "                if self.epsilon > self.final_epsilon and step > self.observe:\n",
    "                    self.epsilon -= (self.start_epsilon- self.final_epsilon) / self.observe\n",
    "\n",
    "                state_t1, reward_t, is_finished = self.game.do(action_idx, object_idx)\n",
    "                self.memory.append((state_t, action_t, object_t, reward_t, state_t1, is_finished))\n",
    "                if step > self.observe:\n",
    "                    batch = random.sample(self.memory, self.batch_size)\n",
    "                    s = [mem[0] for mem in batch]\n",
    "                    a = [mem[1] for mem in batch]\n",
    "                    o = [mem[2] for mem in batch]\n",
    "                    r = [mem[3] for mem in batch]\n",
    "                    s2 = [mem[4] for mem in batch]\n",
    "                    finished = [mem[5] for mem in batch]\n",
    "                    if r > 0:\n",
    "                        win_count += 1\n",
    "\n",
    "                    pred_reward = self.pred_reward.eval(feed_dict={self.inputs: s2})\n",
    "                    action = np.zeros(self.num_action)\n",
    "                    object_= np.zeros(self.num_object)\n",
    "\n",
    "                    _, loss, summary_str = self.sess.run([self.optim, self.loss, self.merged_sum], feed_dict={\n",
    "                    self.inputs: s,\n",
    "                    self.true_reward: a,\n",
    "                    self.pred_reward: pred_reward,\n",
    "                    self.true_object: o,\n",
    "                    self.pred_object: pred_object,\n",
    "                      })\n",
    "                    if step % 10000 == 0:\n",
    "                        self.save(checkpoint_dir, step)\n",
    "\n",
    "                    if step % 50 == 0:\n",
    "                        print(\"Step: [%2d/%7d] time: %4.4f, loss: %.8f, win: %4d\" % (step, self.max_iter, time.time() - start_time, loss, win_count))\n",
    "\n",
    "                if is_finished:\n",
    "                    state_t, reward, is_finished = self.game.new_game()\n",
    "\n",
    "                state_t = state_t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
