{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirzechlucifer/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import textworld.gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import nltk\n",
    "import string\n",
    "import time\n",
    "from tensorflow.contrib.rnn.python.ops import core_rnn_cell\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "                    ________  ________  __    __  ________\n",
      "                   |        \\|        \\|  \\  |  \\|        \\\n",
      "                    \\$$$$$$$$| $$$$$$$$| $$  | $$ \\$$$$$$$$\n",
      "                      | $$   | $$__     \\$$\\/  $$   | $$\n",
      "                      | $$   | $$  \\     >$$  $$    | $$\n",
      "                      | $$   | $$$$$    /  $$$$\\    | $$\n",
      "                      | $$   | $$_____ |  $$ \\$$\\   | $$\n",
      "                      | $$   | $$     \\| $$  | $$   | $$\n",
      "                       \\$$    \\$$$$$$$$ \\$$   \\$$    \\$$\n",
      "              __       __   ______   _______   __        _______\n",
      "             |  \\  _  |  \\ /      \\ |       \\ |  \\      |       \\\n",
      "             | $$ / \\ | $$|  $$$$$$\\| $$$$$$$\\| $$      | $$$$$$$\\\n",
      "             | $$/  $\\| $$| $$  | $$| $$__| $$| $$      | $$  | $$\n",
      "             | $$  $$$\\ $$| $$  | $$| $$    $$| $$      | $$  | $$\n",
      "             | $$ $$\\$$\\$$| $$  | $$| $$$$$$$\\| $$      | $$  | $$\n",
      "             | $$$$  \\$$$$| $$__/ $$| $$  | $$| $$_____ | $$__/ $$\n",
      "             | $$$    \\$$$ \\$$    $$| $$  | $$| $$     \\| $$    $$\n",
      "              \\$$      \\$$  \\$$$$$$  \\$$   \\$$ \\$$$$$$$$ \\$$$$$$$\n",
      "\n",
      "Welcome to another life changing game of TextWorld! Here is how to play! First\n",
      "off, if it's not too much trouble, I need you to go to the east. After that,\n",
      "make an effort to move south. Following that, retrieve the latchkey from the\n",
      "toolbox. Once you have picked up the latchkey, take a trip north. After that,\n",
      "insert the latchkey into the spherical locker. And once you've done that, you\n",
      "win!\n",
      "\n",
      "-= Scullery =-\n",
      "You find yourself in a scullery. A typical one.\n",
      "\n",
      "You scan the room for a counter, and you find a counter. The counter appears to\n",
      "be empty. It would have been so cool if there was stuff on the counter.\n",
      "\n",
      "There is a closed gateway leading south. There is an open door leading east.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request_infos=textworld.EnvInfos(admissible_commands=True,entities=True,description=True,basics=True,command_templates=True,max_episode_steps=50)\n",
    "env_id=textworld.gym.register_game(\"/home/sirzechlucifer/tw_games/custom_game.ulx\",request_infos)\n",
    "env=gym.make(env_id)\n",
    "obs, infos = env.reset() \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['close {c}',\n",
       " 'close {d}',\n",
       " 'drop {o}',\n",
       " 'eat {f}',\n",
       " 'examine {d}',\n",
       " 'examine {o}',\n",
       " 'examine {t}',\n",
       " 'go east',\n",
       " 'go north',\n",
       " 'go south',\n",
       " 'go west',\n",
       " 'insert {o} into {c}',\n",
       " 'inventory',\n",
       " 'lock {c} with {k}',\n",
       " 'lock {d} with {k}',\n",
       " 'look',\n",
       " 'open {c}',\n",
       " 'open {d}',\n",
       " 'put {o} on {s}',\n",
       " 'take {o}',\n",
       " 'take {o} from {c}',\n",
       " 'take {o} from {s}',\n",
       " 'unlock {c} with {k}',\n",
       " 'unlock {d} with {k}']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos[\"command_templates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gateway',\n",
       " 'door',\n",
       " 'spherical locker',\n",
       " 'toolbox',\n",
       " 'type Z box',\n",
       " 'spherical keycard',\n",
       " 'type Z passkey',\n",
       " 'insect',\n",
       " 'counter',\n",
       " 'shelf',\n",
       " 'latchkey',\n",
       " 'frisbee',\n",
       " 'north',\n",
       " 'south',\n",
       " 'east',\n",
       " 'west']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos[\"entities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working code\n",
    "        Shape and command updates left.\n",
    "        Parameter updates left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDQN():\n",
    "    def __init__(self, game, rnn_size=1250, batch_size=25,\n",
    "               seq_length=200, embed_dim=200, layer_depth=3,\n",
    "               start_epsilon=1, epsilon_end_time=1000000,\n",
    "               memory_size=1000000, \n",
    "               checkpoint_dir=\"checkpoint\", forward_only=False):\n",
    "        self.rnn_size = rnn_size\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_depth = layer_depth\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = 1251\n",
    "\n",
    "        self.epsilon = self.start_epsilon = start_epsilon\n",
    "        self.final_epsilon = 0.05\n",
    "        self.observe = 500\n",
    "        self.explore = 500\n",
    "        self.gamma = 0.99\n",
    "        self.num_action_per_step = 1\n",
    "        self.memory_size = memory_size\n",
    "        self.game = game\n",
    "        self._attrs = ['epsilon', 'final_epsilon', 'oberve','explore', 'gamma', 'memory_size', 'batch_size']\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, [1,200])\n",
    "        embed = tf.get_variable(\"embed\", [self.vocab_size, self.embed_dim])\n",
    "        word_embeds = tf.nn.embedding_lookup(embed, self.inputs)\n",
    "        self.cell = tf.nn.rnn_cell.LSTMCell(self.rnn_size)\n",
    "        self.stacked_cell = tf.nn.rnn_cell.MultiRNNCell([self.cell] * self.layer_depth)\n",
    "\n",
    "        outputs, _ = tf.nn.static_rnn(self.cell,\n",
    "        [tf.reshape(embed_t, [1, self.embed_dim]) for embed_t in tf.split( word_embeds, self.seq_length,1)],\n",
    "                            dtype=tf.float32)\n",
    "\n",
    "        output_embed = tf.transpose(tf.stack(outputs), [1, 0, 2])\n",
    "        mean_pool = tf.nn.relu(tf.reduce_mean(output_embed, 1))\n",
    "        # Action scorer. no bias in paper\n",
    "        with tf.variable_scope('action'):\n",
    "            self.pred_reward =core_rnn_cell._linear(mean_pool, len(infos[\"command_templates\"]),0.0)\n",
    "        with tf.variable_scope('object'):\n",
    "            self.pred_object =core_rnn_cell._linear(mean_pool, len(infos[\"entities\"]), 0.0)\n",
    "    \n",
    "        self.true_reward = tf.placeholder(tf.float32, [self.batch_size, len(infos[\"command_templates\"])])\n",
    "        self.true_object = tf.placeholder(tf.float32, [self.batch_size, len(infos[\"entities\"])])\n",
    "\n",
    "        _ = tf.summary.histogram(\"mean_pool\", mean_pool)\n",
    "        _ = tf.summary.histogram(\"pred_reward\", self.pred_reward)\n",
    "        _ = tf.summary.histogram(\"true_reward\", self.true_reward)\n",
    "\n",
    "        _ = tf.summary.scalar(\"pred_reward_mean\", tf.reduce_mean(self.pred_reward))\n",
    "        _ = tf.summary.scalar(\"true_reward_mean\", tf.reduce_mean(self.true_reward))\n",
    "    \n",
    "    \n",
    "    def vectorize(self, text):\n",
    "        null_idx = (len(self.game.observation_space.w2id))\n",
    "        vector = np.ones(self.seq_length) * null_idx\n",
    "        cnt = 0\n",
    "        word_full=[]\n",
    "        words=text.split()\n",
    "        for i in words:\n",
    "            if(i is not string.punctuation and not (\".\" in i) and not ((\",\") in i)):\n",
    "                word_full.append(str(i).lower())\n",
    "            if(\".\" in i):\n",
    "                word_full.append(i.split(\".\")[0])\n",
    "            if(\",\" in i):\n",
    "                word_full.append(i.split(\",\")[0])\n",
    "        for word in word_full:\n",
    "            try:\n",
    "                vector[cnt] = self.game.observation_space.w2id[word]\n",
    "                cnt += 1\n",
    "            except:\n",
    "                continue\n",
    "            return vector\n",
    "    def train(self, max_iter=1000000,\n",
    "        alpha=0.01, learning_rate=0.001,\n",
    "        start_epsilon=1.0, final_epsilon=0.05, memory_size=5000,\n",
    "            checkpoint_dir=\"checkpoint\"):\n",
    "        init=tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            state_t, infos = env.reset()\n",
    "            self.max_iter = max_iter\n",
    "            self.alpha = alpha\n",
    "            self.learning_rate = learning_rate\n",
    "            self.checkpoint_dir = checkpoint_dir\n",
    "            self.step = 0\n",
    "            self.loss = tf.reduce_sum(tf.square(self.true_reward - self.pred_reward))\n",
    "            self.optim = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            _ = tf.summary.scalar(\"loss\", self.loss)\n",
    "            self.memory = deque()\n",
    "            #action = np.zeros(len(infos[\"command_templates\"]))\n",
    "            #action[0] = 1\n",
    "            start_time = time.time()\n",
    "            win_count = 0\n",
    "            print(\" [*] Start\")\n",
    "\n",
    "            for step in range(self.max_iter):\n",
    "                pred_reward, pred_object = sess.run(\n",
    "                [self.pred_reward, self.pred_object], feed_dict={self.inputs: [self.vectorize(infos[\"description\"])]})\n",
    "                #action_t = np.zeros([len(infos[\"command_templates\"])])\n",
    "                #object_t = np.zeros([len(infos[\"entities\"])])\n",
    "\n",
    "        # Epsilon greedy\n",
    "                if random.random() <= self.epsilon or step <= self.observe:\n",
    "                    action_idx = random.randrange(0, len(infos[\"command_templates\"]) - 1)\n",
    "                    object_idx = random.randrange(0, len(infos[\"entities\"]) - 1)\n",
    "                else:\n",
    "                    max_reward = np.max(pred_reward[0])\n",
    "                    max_object = np.max(pred_object[0])\n",
    "                    action_idx = np.random.choice(np.where(pred_reward[0] == max_reward)[0])\n",
    "                    object_idx = np.random.choice(np.where(pred_object[0] == max_object)[0])\n",
    "          #best_q = (max_action + max_object)/2\n",
    "        # run and observe rewards\n",
    "                #action_t[action_idx] = 1\n",
    "                #object_t[object_idx] = 1\n",
    "                \n",
    "                if self.epsilon > self.final_epsilon and step > self.observe:\n",
    "                    self.epsilon -= (self.start_epsilon- self.final_epsilon) / self.observe\n",
    "                print(infos[\"description\"])\n",
    "                state_t1, reward_t, is_finished,infos = self.game.step(infos[\"command_templates\"][action_idx]+infos[\"entities\"][object_idx])\n",
    "                env.render()\n",
    "                self.memory.append((state_t, infos[\"command_templates\"][action_idx], infos[\"entities\"][object_idx], reward_t, state_t1, is_finished))\n",
    "\n",
    "        # qLearnMinibatch : Q-learning updates\n",
    "                if step > self.observe:\n",
    "                    batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "                    s = [mem[0] for mem in batch]\n",
    "                    a = [mem[1] for mem in batch]\n",
    "                    o = [mem[2] for mem in batch]\n",
    "                    r = [mem[3] for mem in batch]\n",
    "                    s2 = [mem[4] for mem in batch]\n",
    "                    finished = [mem[5] for mem in batch]\n",
    "\n",
    "                    if r > 0:\n",
    "                        win_count += 1\n",
    "\n",
    "                    pred_reward = self.pred_reward.eval(feed_dict={self.inputs: s2})\n",
    "\n",
    "                    action = np.zeros(self.num_action)\n",
    "                    object_= np.zeros(self.num_object)\n",
    "\n",
    "                    _, loss, summary_str = sess.run([self.optim, self.loss, self.merged_sum], feed_dict={\n",
    "                    self.inputs: s,\n",
    "                    self.true_reward: a,\n",
    "                    self.pred_reward: pred_reward,\n",
    "                    self.true_object: o,\n",
    "                    self.pred_object: pred_object,\n",
    "                  })\n",
    "\n",
    "                    if step % 50 == 0:\n",
    "                        self.save(checkpoint_dir, step)\n",
    "\n",
    "                    if step % 10 == 0:\n",
    "                        print(\"Step: [%2d/%7d] time: %4.4f, loss: %.8f, win: %4d\" % (step, self.max_iter, time.time() - start_time, loss, win_count))\n",
    "\n",
    "                if is_finished:\n",
    "                    state_t1, infos1 = env.reset()\n",
    "\n",
    "                state_t = state_t1\n",
    "                infos=infos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n"
     ]
    }
   ],
   "source": [
    "obj=LSTMDQN(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
